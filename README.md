# CycleGAN

CycleGAN is a generative model designed for image-to-image translation, particularly effective in situations where paired datasets are not available. This model enables the transformation of images from one domain to another without the need for direct correspondence between images in the two domains. It operates with two sets of images, each from a different domain, and learns to translate images from one set into the style of the other set while preserving the content.

The CycleGAN model consists of two generators and two discriminators, which work together to translate images between two different domains. The generators G and F are tasked with learning the mapping between these domains:

- **G : X → Y** translates images from domain X (PAS) to domain Y (IHC).
- **F : Y → X** translates images from domain Y (IHC) back to domain X (PAS).

The goal of the generators is to produce images that are indistinguishable from those in the target domain, while the discriminators Dx and Dy are trained to distinguish between real images from the target domain and those generated by the model. This adversarial relationship between the generators and discriminators allows the CycleGAN to effectively translate unpaired images between domains, ensuring that the content is preserved while the style is adapted to match the target domain.

## Project Overview

In my project, I trained a CycleGAN model on unpaired IHC-stained and PAS-stained glomeruli tiles. The objective was to translate PAS-stained images, which traditionally do not highlight podocytes, into IHC-like images that better reveal these structures. The training was done unconditionally, meaning it did not involve any specific masks identifying podocytes during the process. Instead, the model learned the general transformation between the two staining techniques.

The CycleGAN model effectively captured the style transformation from PAS to IHC, with an evaluation showing a **Structural Similarity Index (SSIM)** of 0.92. This high SSIM score indicates that the translated images closely resemble the real IHC images, making it a promising approach for enhancing the utility of PAS-stained slides in podocyte analysis.

## Segmentation Using Synthetic IHC Images

The process of using CycleGAN to generate IHC-like images from PAS-stained images was conducted unconditionally, meaning the model was not specifically trained to identify podocytes or other structures within the nuclei. Consequently, we did not anticipate CycleGAN to accurately highlight podocytes in the generated IHC images. 

Despite this, we proceeded to segment these synthetic IHC images using the previously trained IHC-based Attention U-Net model. The segmentation results were evaluated against hand-annotated podocyte regions on the original PAS images. While the IHC-based U-Net effectively segmented podocytes by accurately identifying areas highlighted by the DAB stain color in the synthetic IHC images, they weren’t true podocytes. This led to lower performance metrics—such as: F1 Score: 0.228 , Precision: 0.209, Recall: 0.252, Mean IoU: 0.129, Accuracy: 0.933

## Conclusion

This experiment demonstrates that CycleGAN can translate from PAS to IHC and provide images with almost the same texture as the original IHCs. Furthermore, these synthetic IHC images can also be segmented by an Attention U-Net model trained solely on real IHC images, though with reduced accuracy due to the absence of true podocytes in the synthetic images.
